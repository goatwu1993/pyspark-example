{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"My-App\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basics Adding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums1 = range(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums1Rdd = sc.parallelize(nums1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "evens = nums1Rdd.map(lambda x: x-(x%3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums1Tuple = evens.map(lambda x: (x,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 10),\n",
       " (0, 10),\n",
       " (0, 10),\n",
       " (3, 10),\n",
       " (3, 10),\n",
       " (3, 10),\n",
       " (6, 10),\n",
       " (6, 10),\n",
       " (6, 10),\n",
       " (9, 10)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums1Tuple.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 30), (9, 10), (6, 30), (3, 30)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums1Tuple.reduceByKey(lambda x, y: x + y).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyrics = [\"\"\"Look at the stars\n",
    "Look how they shine for you\n",
    "And everything you do\n",
    "Yeah they were all yellow\n",
    "I came along\n",
    "I wrote a song for you\n",
    "And all the things you do\n",
    "And it was called \"Yellow\"\n",
    "So then I took my turn\n",
    "Oh what a thing to have done\n",
    "And it was all yellow\n",
    "Your skin\n",
    "Oh yeah, your skin and bones\n",
    "Turn into something beautiful\n",
    "You know, you know I love you so\n",
    "You know I love you so\n",
    "I swam across\n",
    "I jumped across for you\n",
    "Oh what a thing to do\n",
    "'Cause you were all yellow\n",
    "I drew a line\n",
    "I drew a line for you\n",
    "Oh what a thing to do\n",
    "And it was all yellow\n",
    "Your skin\n",
    "Oh yeah your skin and bones\n",
    "Turn into something beautiful\n",
    "And you know\n",
    "For you I'd bleed myself dry\n",
    "For you I'd bleed myself dry\n",
    "It's true\n",
    "Look how they shine for you\n",
    "Look how they shine for you\n",
    "Look how they shine for\n",
    "Look how they shine for you\n",
    "Look how they shine for you\n",
    "Look how they shine\n",
    "Look at the stars\n",
    "Look how they shine for you\n",
    "And all the things that you do\"\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyricsRdd = sc.parallelize(lyrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "lyricsFlat = lyricsRdd.flatMap(lambda x: re.split('; |, |\\*|\\n| ',x.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lyricsTuple = lyricsFlat.map(lambda x: (x,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "WordCounts = lyricsTuple.reduceByKey(lambda x,y: x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('you', 21),\n",
       " ('for', 12),\n",
       " ('look', 10),\n",
       " ('i', 9),\n",
       " ('and', 9),\n",
       " ('they', 9),\n",
       " ('how', 8),\n",
       " ('shine', 8),\n",
       " ('a', 6),\n",
       " ('all', 6)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "WordCounts.sortBy(lambda x: -x[1]).take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From Spark Website\n",
    "> **Note**: If you are grouping in order to perform an aggregation (such as a sum or average) over each key, using reduceByKey or aggregateByKey will yield much better performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "\n",
    "  - <https://spark.apache.org/docs/2.1.1/programming-guide.html#rdd-operations>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('conda-env': conda)",
   "language": "python",
   "name": "python37464bitcondaenvconda3f4d7783b740440a9c80fd58cd6c0e7e"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
