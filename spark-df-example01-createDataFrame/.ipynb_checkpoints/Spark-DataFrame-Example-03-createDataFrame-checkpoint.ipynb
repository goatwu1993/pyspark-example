{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"My-App\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SparkSession.createDataFrame**: Creates a DataFrame from an RDD, a list or a pandas.DataFrame.\n",
    "\n",
    "**Parameters**:\n",
    "  - data: rdd, list or pandas.DataFrame\n",
    "  - schema:  \n",
    "    \n",
    "    - pyspark.sql.types.DataType\n",
    "    - datatype string\n",
    "    - list of column names  \n",
    "    \n",
    "  - samplingRatio\n",
    "  - verifySchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "people_id_list = [('Alice',1),\n",
    "    ('Bob',2),\n",
    "    ('Cindy',3)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_1='Alice', _2=1), Row(_1='Bob', _2=2), Row(_1='Cindy', _2=3)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_people_noschema = spark.createDataFrame(people_id_list)\n",
    "df_people_noschema.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='Alice', age=1), Row(name='Bob', age=2), Row(name='Cindy', age=3)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_people = spark.createDataFrame(\n",
    "    people_id_list,\n",
    "    ['name', 'age'])\n",
    "df_people.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='Alice', age=1), Row(name='Bob', age=2), Row(name='Cindy', age=3)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_people_2 = spark.createDataFrame(\n",
    "    data=people_id_list,\n",
    "    schema=['name', 'age'])\n",
    "df_people_2.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/peteeelol/anaconda3/envs/conda-env/lib/python3.7/site-packages/pyspark/sql/session.py:346: UserWarning: inferring schema from dict is deprecated,please use pyspark.sql.Row instead\n",
      "  warnings.warn(\"inferring schema from dict is deprecated,\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(age=1, name='Alice')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = [{'name': 'Alice', 'age': 1}]\n",
    "spark.createDataFrame(d).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(age=1, name='Alice'), Row(age=2, name='Bob')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = [{'name': 'Alice', 'age': 1},\n",
    "    {'name': 'Bob', 'age': 2}]\n",
    "spark.createDataFrame(d).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This is NOT GOOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(age=[1, 2], name=['Alice', 'Bob'])]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = [{'name': ['Alice','Bob'], 'age': [1,2]}]\n",
    "spark.createDataFrame(d).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='Alice', age=1), Row(name='Bob', age=2), Row(name='Cindy', age=3)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = spark.sparkContext\n",
    "spark.createDataFrame(\n",
    "    people_id_list,\n",
    "    ['name', 'age']\n",
    ").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='0', age=0),\n",
       " Row(name='1', age=1),\n",
       " Row(name='2', age=2),\n",
       " Row(name='3', age=3),\n",
       " Row(name='4', age=4),\n",
       " Row(name='5', age=5),\n",
       " Row(name='6', age=6),\n",
       " Row(name='7', age=7),\n",
       " Row(name='8', age=8),\n",
       " Row(name='9', age=9)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize(range(10))\n",
    "rdd2 = rdd.map(lambda x : (str(x),x))\n",
    "spark.createDataFrame(\n",
    "    rdd2,\n",
    "    ['name', 'age']).\\\n",
    "collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROW (pyspark.sql.types.Row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Row(name, age)>\n",
      "<class 'pyspark.sql.types.Row'>\n"
     ]
    }
   ],
   "source": [
    "Person = Row('name', 'age')\n",
    "print(Person)\n",
    "print(type(Person))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Alice', 1), ('Bob', 2), ('Cindy', 3)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people_id_rdd = sc.parallelize(people_id_list)\n",
    "people_id_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.PipelinedRDD'>\n"
     ]
    }
   ],
   "source": [
    "people_rdd = people_id_rdd.map(lambda r: Person(*r))\n",
    "print(type(people_rdd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='Alice', age=1), Row(name='Bob', age=2), Row(name='Cindy', age=3)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_1='Alice', _2=1), Row(_1='Bob', _2=2), Row(_1='Cindy', _2=3)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people = spark.createDataFrame(people_id_rdd)\n",
    "people.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StructType & Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "person_schema = StructType([\n",
    "    StructField(\"name\",StringType(),True),\n",
    "    StructField(\"age\",IntegerType(),True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = spark.createDataFrame(people_id_rdd, person_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='Alice', age=1), Row(name='Bob', age=2), Row(name='Cindy', age=3)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='Alice', age=1), Row(name='Bob', age=2), Row(name='Cindy', age=3)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.createDataFrame(df_people.toPandas()).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(0=1, 1=2)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas\n",
    "spark.createDataFrame(pandas.DataFrame([[1, 2]])).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schema by string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(a='Alice', b=1), Row(a='Bob', b=2), Row(a='Cindy', b=3)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.createDataFrame(people_id_rdd, \"a: string, b: int\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_rdd = people_id_rdd.map(lambda row: row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(value=1), Row(value=2), Row(value=3)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.createDataFrame(id_rdd, \"int\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_false_rdd = sc.parallelize([True,False,False,True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(value=True), Row(value=False), Row(value=False), Row(value=True)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.createDataFrame(true_false_rdd, \"boolean\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(value='true'), Row(value='false'), Row(value='false'), Row(value='true')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.createDataFrame(true_false_rdd, \"string\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "  - <https://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html#pyspark.sql.SparkSession>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('conda-env': conda)",
   "language": "python",
   "name": "python37464bitcondaenvconda3f4d7783b740440a9c80fd58cd6c0e7e"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
