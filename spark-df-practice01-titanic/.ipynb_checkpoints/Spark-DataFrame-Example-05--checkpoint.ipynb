{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"My-App\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "# Create Dataframe\n",
    "sdf_train = spark.read.format(\"csv\").option(\"header\",\"true\").load(\"train.csv\")\n",
    "sdf_test = spark.read.format(\"csv\").option(\"header\",\"true\").load(\"test.csv\")\n",
    "\n",
    "# createOrReplaceTempView TempView\n",
    "#sdf_train.createOrReplaceTempView(\"sdf_train\")\n",
    "#sdf_test.createOrReplaceTempView(\"sdf_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age has 177 N/A out of 891 at train\n",
      "Age has 86 N/A out of 418 at test\n",
      "Cabin has 687 N/A out of 891 at train\n",
      "Cabin has 327 N/A out of 418 at test\n",
      "Embarked has 2 N/A out of 891 at train\n",
      "Embarked has 0 N/A out of 418 at test\n",
      "[Row(Cabin=None), Row(Cabin='C85'), Row(Cabin=None), Row(Cabin='C123'), Row(Cabin=None)]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "for i in sdf_test.columns:\n",
    "    if sdf_train.where(col(i).isNull()).count() or sdf_train.where(col(i).isNull()).count():\n",
    "        print(f'{i} has {sdf_train.where(col(i).isNull()).count()} N/A out of {sdf_train.count()} at train')\n",
    "        print(f'{i} has {sdf_test.where(col(i).isNull()).count()} N/A out of {sdf_test.count()} at test')\n",
    "print(sdf_train.select(\"Cabin\").limit(5).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "687"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sdf_train.where(col('Cabin').isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age has 177 N/A out of 891 at train\n",
      "Age has 86 N/A out of 418 at test\n",
      "Cabin has 687 N/A out of 891 at train\n",
      "Cabin has 327 N/A out of 418 at test\n",
      "Embarked has 2 N/A out of 891 at train\n",
      "Embarked has 0 N/A out of 418 at test\n",
      "[Row(Cabin=None), Row(Cabin='C85'), Row(Cabin=None), Row(Cabin='C123'), Row(Cabin=None)]\n"
     ]
    }
   ],
   "source": [
    "for i in sdf_test.columns:\n",
    "    if sdf_train.where(col(i).isNull()).count() or sdf_train.where(col(i).isNull()).count():\n",
    "        print(f'{i} has {sdf_train.where(col(i).isNull()).count()} N/A out of {sdf_train.count()} at train')\n",
    "        print(f'{i} has {sdf_test.where(col(i).isNull()).count()} N/A out of {sdf_test.count()} at test')\n",
    "print(sdf_train.select(\"Cabin\").limit(5).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(Cabin='None'), Row(Cabin='C85'), Row(Cabin='None'), Row(Cabin='C123'), Row(Cabin='None')]\n"
     ]
    }
   ],
   "source": [
    "sdf_train = sdf_train.fillna({'Cabin':'None'})\n",
    "sdf_test = sdf_test.fillna({'Cabin':'None'})\n",
    "sdf_train.createOrReplaceTempView(\"sdf_train\")\n",
    "sdf_test.createOrReplaceTempView(\"sdf_test\")\n",
    "\n",
    "print(sdf_train.select(\"Cabin\").limit(5).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age has 177 N/A out of 891 at train\n",
      "Age has 86 N/A out of 418 at test\n",
      "Embarked has 2 N/A out of 891 at train\n",
      "Embarked has 0 N/A out of 418 at test\n"
     ]
    }
   ],
   "source": [
    "for i in sdf_test.columns:\n",
    "    if sdf_train.where(col(i).isNull()).count() or sdf_train.where(col(i).isNull()).count():\n",
    "        print(f'{i} has {sdf_train.where(col(i).isNull()).count()} N/A out of {sdf_train.count()} at train')\n",
    "        print(f'{i} has {sdf_test.where(col(i).isNull()).count()} N/A out of {sdf_test.count()} at test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Enginnering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "foo = sdf_train.where(sdf_train.Sex == \"female\"). \\\n",
    "    where(~sdf_train.Name.rlike('Mrs')).\\\n",
    "    where(~sdf_train.Name.rlike('Miss')).\\\n",
    "    count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "name_regex = {\n",
    "#female_regex\n",
    "    \"Name_Mrs\": '(Mrs|Mme|Dona|Countess)',\n",
    "    \"Name_Miss\": '(Mlle|Miss)',\n",
    "#male_regex \n",
    "    \"Name_Mr\": '(Don|Mr\\.|Jonkheer)',\n",
    "    \"Name_Master\": '(Mlle|Miss)',\n",
    "    \"Name_Dr\": '(Dr)',\n",
    "    \"Name_Rev\": '(Rev)',\n",
    "    \"Name_Soldier\": '(Major|Col|Capt)'\n",
    "}\n",
    "\n",
    "for new_col in name_regex.keys():\n",
    "    sdf_train = sdf_train.withColumn(new_col, \\\n",
    "                                     sdf_train. \\\n",
    "                                         Name. \\\n",
    "                                         rlike(name_regex[new_col]). \\\n",
    "                                         cast(IntegerType()))\n",
    "    sdf_test = sdf_test.withColumn(new_col, \\\n",
    "                                   sdf_test. \\\n",
    "                                       Name. \\\n",
    "                                       rlike(name_regex[new_col]). \\\n",
    "                                       cast(IntegerType()))\n",
    "\n",
    "sdf_train.createOrReplaceTempView(\"sdf_train\")\n",
    "sdf_test.createOrReplaceTempView(\"sdf_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ticket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql.functions import element_at,log1p\n",
    "\n",
    "\n",
    "sdf_train = sdf_train.\\\n",
    "        withColumn(\"tmp\", split(\"Ticket\", \" \")).\\\n",
    "        withColumn(\"last_element\", element_at(col('tmp'), -1,)).\\\n",
    "        withColumn(\"Ticket_Num_Log\", log1p(col('last_element'))).\\\n",
    "        drop(\"tmp\",\"last_element\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticket_regex = {\n",
    "    \"Ticket_PC\": '(PC)',\n",
    "    \"Ticket_TON\": '(TON)',\n",
    "    \"Ticket_NO_PREFIEX\": '(^[0-9])'\n",
    "}\n",
    "\n",
    "for new_col in ticket_regex.keys():\n",
    "    sdf_train = sdf_train.withColumn(new_col, sdf_train[\"Ticket\"].rlike(ticket_regex[new_col]).cast(IntegerType()))\n",
    "    sdf_test = sdf_test.withColumn(new_col, sdf_test[\"Ticket\"].rlike(ticket_regex[new_col]).cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cabin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cabin_regex = {\n",
    "    \"Cabin_A\": \"^A\",\n",
    "    \"Cabin_B\": \"^B\",\n",
    "    \"Cabin_C\": \"^C\",\n",
    "    \"Cabin_D\": \"^D\",\n",
    "    \"Cabin_E\": \"^E\",\n",
    "    \"Cabin_F\": \"^F\",\n",
    "    \"Cabin_G\": \"^G\",\n",
    "    \"Cabin_T\": \"^T\",\n",
    "    \"Cabin_None\": \"None\"\n",
    "}\n",
    "\n",
    "for new_col in cabin_regex.keys():\n",
    "    sdf_train = sdf_train.withColumn(new_col, sdf_train[\"Cabin\"].rlike(cabin_regex[new_col]).cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Later need one-hot\n",
    "cols_int_to_str = [\"Pclass\",\"SibSp\",\"Parch\"]\n",
    "for cols in cols_int_to_str:\n",
    "    sdf_train = sdf_train.withColumn(cols, sdf_train[cols].cast(\"string\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Age'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mylist = sdf_train.columns\n",
    "mylist.pop(mylist.index(\"Name\"))\n",
    "mylist.pop(mylist.index(\"Sex\"))\n",
    "mylist.pop(mylist.index(\"Embarked\"))\n",
    "mylist.pop(mylist.index(\"Age\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age has 177 N/A out of 891 at train\n",
      "Age has 86 N/A out of 418 at test\n",
      "Embarked has 2 N/A out of 891 at train\n",
      "Embarked has 0 N/A out of 418 at test\n",
      "[Row(Cabin='None'), Row(Cabin='C85'), Row(Cabin='None'), Row(Cabin='C123'), Row(Cabin='None')]\n"
     ]
    }
   ],
   "source": [
    "for i in sdf_test.columns:\n",
    "    if sdf_train.where(col(i).isNull()).count() or sdf_train.where(col(i).isNull()).count():\n",
    "        print(f'{i} has {sdf_train.where(col(i).isNull()).count()} N/A out of {sdf_train.count()} at train')\n",
    "        print(f'{i} has {sdf_test.where(col(i).isNull()).count()} N/A out of {sdf_test.count()} at test')\n",
    "print(sdf_train.select(\"Cabin\").limit(5).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mylist = sdf_train.columns.pop(11)\n",
    "\n",
    "for i in mylist:\n",
    "    if str(type(sdf_train.schema[i].dataType)) == \"<class 'pyspark.sql.types.StringType'>\":\n",
    "        #print(i)\n",
    "        sdf_train = sdf_train.withColumn(i, sdf_train[i].cast(\"int\"))\n",
    "for i in mylist:\n",
    "    if str(type(sdf_train.schema[i].dataType)) == \"<class 'pyspark.sql.types.StringType'>\":\n",
    "        print(i)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "def drop_null_columns(df):\n",
    "    \"\"\"\n",
    "    This function drops all columns which contain null values.\n",
    "    :param df: A PySpark DataFrame\n",
    "    \"\"\"\n",
    "    null_counts = df.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in df.columns]).collect()[0].asDict()\n",
    "    to_drop = [k for k, v in null_counts.items() if v > 0]\n",
    "    df = df.drop(*to_drop)\n",
    "    return df\n",
    "           \n",
    "sdf_train = drop_null_columns(sdf_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in sdf_train.columns:\n",
    "    if sdf_train.where(col(i).isNull()).count() :\n",
    "        print(f'{i} has {sdf_train.where(col(i).isNull()).count()} N/A out of {sdf_train.count()} at train')\n",
    "        #print(f'{i} has {sdf_test.where(col(i).isNull()).count()} N/A out of {sdf_test.count()} at test')\n",
    "\n",
    "sdf_train = sdf_train.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_train.dropna().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(traindf, testdf) = sdf_train.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer, OneHotEncoder, VectorAssembler, IndexToString\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Index labels, adding metadata to the label column.\n",
    "# Fit on whole dataset to include all labels in index.\n",
    "genderIndexer = StringIndexer(inputCol=\"Sex\", outputCol=\"indexedSex\")\n",
    "PclassIndexer = StringIndexer(inputCol=\"Pclass\", outputCol=\"indexedPclass\")\n",
    "#genderIndexer = StringIndexer(inputCol=\"SibSp\", outputCol=\"indexedSibSp\")\n",
    "embarkIndexer = StringIndexer(inputCol=\"Embarked\", outputCol=\"indexedEmbarked\")\n",
    " \n",
    "surviveIndexer = StringIndexer(inputCol=\"Survived\", outputCol=\"indexedSurvived\")\n",
    " \n",
    "# One Hot Encoder on indexed features\n",
    "genderEncoder = OneHotEncoder(inputCol=\"indexedSex\", outputCol=\"sexVec\")\n",
    "PclassEncoder = OneHotEncoder(inputCol=\"indexedPclass\", outputCol=\"PclassVec\")\n",
    "embarkEncoder = OneHotEncoder(inputCol=\"indexedEmbarked\", outputCol=\"embarkedVec\")\n",
    "\n",
    "\n",
    "# Create the vector structured data (label,features(vector))\n",
    "assembler = VectorAssembler(inputCols=[\"Pclass\",\"sexVec\",\"Age\",\"SibSp\",\"Fare\",\"embarkedVec\"],outputCol=\"features\")\n",
    "\n",
    "# Train a RandomForest model.\n",
    "rf = RandomForestClassifier(labelCol=\"indexedSurvived\", featuresCol=\"features\")\n",
    " \n",
    "# Chain indexers and forest in a Pipeline\n",
    "pipeline = Pipeline(stages=[surviveIndexer, genderIndexer, PclassIndexer, embarkIndexer, PclassEncoder,embarkEncoder,genderEncoder, assembler, rf]) # genderIndexer,embarkIndexer,genderEncoder,embarkEncoder,\n",
    " \n",
    "\n",
    "model = pipeline.fit(traindf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## References\n",
    "\n",
    "  - <https://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html#pyspark.sql.SparkSession>\n",
    "  - <https://creativedata.atlassian.net/wiki/spaces/SAP/pages/83237142/Pyspark+-+Tutorial+based+on+Titanic+Dataset>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('conda-env': conda)",
   "language": "python",
   "name": "python37464bitcondaenvconda3f4d7783b740440a9c80fd58cd6c0e7e"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
